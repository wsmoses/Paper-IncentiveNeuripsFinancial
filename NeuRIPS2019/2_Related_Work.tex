\section{Related Work}
\label{sec:relatedwork}
Computer science has only recently begun to reckon with the agency of individuals subject to complex automated decision rules, and the incentives they experience. One can understand the problem of identifying incentives as a sub-problem of algorithmic interpretability \cite{guidotti2019survey, doshi2017towards}, though interpretability is a loaded term \cite{lipton2016mythos}, and as we will show in Section \ref{sec:linear} many methods from the literature fail to accurately recover an algorithm's incentives.

Incentives have been studied for a long time in the economics literature, particularly around the misalignment of incentives between different parties \cite{kerr1975folly, grossman1992analysis}. Modeling individuals as subject to MDPs has long been practice in economics \cite{rust1994structural}. For example, \citet{keane1997career} construct a $5$-action MDP to study the career choices of young men, validating their model on real-world data. However, in this literature, agents are subject to reward rules far simpler than today's algorithmic decision functions.

\citet{hardt2016strategic} consider agency through the lens of individuals ``gaming'' the classifier, thus reducing its accuracy. They explore means for reducing the ability of agents to game a classifier, though doing so may unfairly reduce the agency of previously-disadvantaged groups (\citet{milli2019social, hu2019disparate}). Recent work has suggested that this ``effort-unfairness'' across groups may appear even in non-strategic classifiers \cite{heidari2019long}.
On the opposite end, \citet{kleinberg2018classifiers} study the problem of designing a decision rule solely for the purpose of imposing a particular set of incentives on an individual, given a linear model of that individual's available actions.

Recent work has begun to wrestle with the question of extracting incentives from existing decision models that were not trained to consider the behavior of their subjects. \citet{wachter2017counterfactual} propose `explaining' incentives to individuals by identifying (e.g. via MILPs \cite{russell2019efficient} or SMT-solvers \cite{karimi2019model}) a `similar' hypothetical individual who would've received a better decision. This line of work implicitly assumes that an informative counterfactual is ``close'' in feature space to the original, but does not ask what sequence of actions would allow the individual to attain the counterfactual -- or whether such a sequence even exists. \citet{ustun2019actionable} first proposed the question most directly related to ours, of finding actionable ``recourse'' for individuals subject to the decisions of a linear model. Their approach is applicable to categorical/linear transition models and white-box linear decision functions (including linear and logistic regressions), and provides optimal incentives within these settings.

Two independent concurrent works have explored the challenge of identifying multi-step action sequences for differentiable decision functions like neural networks. Much like this work, \citet{Ramakrishnan2019synthesizing} model the problem as choosing a sequence of actions to maximize a differentiable decision function, by exhaustively searching the space of acceptable discrete action sequences, and for each sequence adjusting each action by locally solving a differentiable optimization problem. \citet{Joshi2019towards} learn a latent embedding of the data manifold, and then assume that an individual acts by perturbing their features in this latent space (thus ensuring that the result of each ``step'' yields a feature vector near the original data distribution). They then find an action sequence by taking a series of gradient steps in latent space to approximately maximize the combined embedding and decision models.

To our knowledge, ours is the first work to propose a means of identifying incentivized action sequences for arbitrary black-box (possibly non-differentiable) models, and to have identified the connection between extracting recourse and reinforcement learning.
%\wmnote{we say something similar in intro btw} \yocomment{Consider removing} To the best of our knowledge, no previous work has derived incentives of entirely black-box models given only query-access to the model, or considered incentives in settings with sequential actions.
%, where the order of execution changes the effect of the action on the ultimate decision.