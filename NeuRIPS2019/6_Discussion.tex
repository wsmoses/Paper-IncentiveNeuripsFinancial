\section{Discussion}
\label{sec:discussion}

In this work, we have proposed a framework for reasoning about the incentives of individuals subject to black box decisions, by formulating their choices as an MDP. We demonstrated the framework's ability to analyze the agency of different groups subject to a recidivism predictor, and illustrated the framework's ability to extract superior advice policies for a customer subject to FICO's credit simulator.

% First, we demonstrated the use of this framework to identify incentivized actions from black-box decision-making functions. This is best exemplified by the pretrial risk assessment experiment in which we demonstrate how the model's use of race and gender reduces a person's agency.
% Secondly, we demonstrate how this framework can be used to audit the advice provided by a black box decision (either given explicitly from the black box, or implicitly in the form of incentivized actions). This is best exemplified by the FICO score experiment where we show that traditional ``greedy'' style advice yields suboptimal outcomes.
%We demonstrate that by defining available actions and their resulting transitions in two real-world decision settings, we are able to provide individuals with closer-to-optimal advice policies.
Interestingly, this framework can be used for two different ends: comparing the utility of different advice-generating algorithms (e.g. local approximation, MDP-based tree search) on a fixed decision model, and comparing the incentives generated by different decision models while keeping the advice-generating algorithm fixed. We suggest that it may be most useful to choose an advice-generating scheme first (either based on MDP performance guarantees, or qualities like human-interpretability). Once a scheme has been chosen, we can reasonably compare the incentives/agency that different decision models provide to agents who we assume will follow that same advice scheme. 

Throughout this work we have assumed that a decision-making model does not change based on an individual's actions. However, when many individuals follow incentives at once, this is liable to shift the data distribution, and thus the model. The literature on strategic classification responds by attempting to remove agency from the individual. One interesting open question is whether we can incorporate the impact of subjects' actions on the decision-maker in such a way that the decision-maker will continue to provide agency to those subjects.

% REINSERT IN FINAL DRAFT?
% A potential drawback to publicly communicating a model's incentivized actions is that if over time a decision-rule changes, individuals who expended effort to follow these previously-incentivized actions may find themselves having wasted their resources, or perhaps even in a worse state than they began. To this end, it is important that decision-makers communicate to individuals which incentives they expect to remain valid into the future, and to constrain future decision-rules to maintain those same incentives.

One primary ethical danger looms over the study of algorithmic incentives. What if focusing on how an individual could improve the decision they receive from an existing model will distract us from focusing on larger questions, namely: is the basis of the decision correct? And more importantly, does the decision-maker have a right to dictate our actions?

These are vital questions for practitioners to consider. We take solace in a simple truth:
%two observations. If the basis of a decision-rule were purely to incentivize particular actions, \cite{kleinberg2018classifiers} suggest that that could be accomplished with even a linear decision function. Thus, the primary utility of studying the incentives of complex functions is in discovering the unintentional secondary incentives of systems primarily trained for a different goal, like accuracy.  
a first step in the process of eliminating damaging incentives is having the tools to surface those incentives, so that they can be confronted and expunged.